	\documentclass[twoside]{article}
\usepackage{../../estilo-ejercicios}

%--------------------------------------------------------
\begin{document}

\title{Ejercicios de From Calculus to Cohomology, Capítulo 2}
\author{Javier Aguilar Martín}
\maketitle


\begin{ejercicio}{2.2}
Econtrar $w\in Alt^2(\R^4)$ tal que $w\land w\neq 0$.
\end{ejercicio}
\begin{solucion}
Sea $w=dx^1\land dx^2 +dx^3\land dx^4$. Entonces
$$w\land w= dx^1\land dx^2\land dx^3\land dx^4 +dx^3\land dx^4\land dx^1\land dx^2=2dx^1\land dx^2\land dx^3\land dx^4 \neq 0.$$
\end{solucion}

\newpage

\begin{ejercicio}{2.3}
Demostrar que existen isomorfismos
\begin{align*}
&\R^3\overset{i}{\to} Alt^1(\R^3), &\R^3\overset{j}{\to} Alt^2(\R^3)
\end{align*}
dados por
\begin{align*}
& i(v)(w)=v\cdot w, & j(v)(w_1,w_2)=\det(v,w_1,w_2).
\end{align*}
Demostrar que para $v_1,v_2\in\R^3$ se tiene
$$i(v_1)\land i(v_2)=j(v_1\times v_2).$$
\end{ejercicio}
\begin{solucion}
Por las propiedades del producto escalar y del determinante respectivamente, tanto $i$ como $j$ son homomorfismos de espacios vectoriales. Como $\dim(\R^3)=\dim(Alt^1(\R^3))=\dim(Alt^2(\R^3))=3$, basta probar que $i$ y $j$ son inyectivas o sobreyectivas para tener que son isomorfismos.

\underline{$i$ es inyectiva}:
Sean $v_1,v_2\in\R^3$ tales que $i(v_1)=i(v_2)$. Esto quiere decir que para cualquier $w\in\R^3$, $i(v_1)(w)=i(v_2)(w)$. En particular tomando $w_i=e_i$ tenemos que $v_1=v_2$. 


\underline{$j$ es inyectiva}:
Sean $v_1,v_2\in\R^3$ tales que $j(v_1)=j(v_2)$. Esto quiere decir que para cualesquiera $w_1,w_2\in\R^3$, $j(v_1)(w_1,w_2)=j(v_2)(w1,w_2)$. Tomando las parejas $(e_1,e_2)$, $(e_1,e_3)$ y $(e_2,e_3)$ se obtiene que $v_1=v_2$. 

Por último, sean $w_1,w_2\in\R^3$. Veamos que se tiene la última igualdad.
$$(i(v_1)\land i(v_2))(w_1,w_2)=\begin{vmatrix}
i(v_1)(w_1) & i(v_1)(w_2)\\
i(v_2)(w_1) & i(v_2)(w_2)
\end{vmatrix}=\begin{vmatrix}
v_1\cdot w_1 & v_1\cdot w_2\\
v_2\cdot w_1 & v_2\cdot w_2
\end{vmatrix}=(v_1\cdot w_1)(v_2\cdot w_2)-(v_1\cdot w_2)(v_2\cdot w_1).$$
$$j(v_1\times v_2)(w_1,w_2)=\begin{vmatrix}
v_1\times v_2\\
w_1\\
w_2
\end{vmatrix}=(v_1\times v_2)\cdot (w_1\times w_2)=(v_1\cdot w_1)(v_2\cdot w_2)-(v_1\cdot w_2)(v_2\cdot w_1).$$
Por lo que se tiene la igualdad. 
\end{solucion}

\newpage

\begin{ejercicio}{2.4}
Sea $V$ un $\R$-espacio vectorial de dimensión finita con producto escalar y sea 
$$i:V\to V^*=Alt^1(V)$$
la aplicación $\R$-lineal dada por 
$$i(v)(w)=v\cdot w.$$
Probar que si $\{b_1,\dots, b_n\}$ es una base ortonormal de $V$, entonces
$$i(b_k)=b_k^*,$$
donde $\{b_1^*,\dots, b_n^*\}$ es la base dual. Concluir que $i$ es un isomorfismo.
\end{ejercicio}
\begin{solucion}
Basta probar que $i(b_k)(b_j)=\delta_{kj}$. 
\begin{gather*}
i(b_k)(b_k)=b_kb_k=|b_k|^2=1\\
i(b_k)(b_j)=b_kb_j=0, \forall j\neq k.
\end{gather*}
Como $i$ transforma bases en bases, es un isomorfismo.
\end{solucion}

\newpage

\begin{ejercicio}{2.5}
Con las mismas hipótesis que en el ejercicio anterior, probar la existencia de un producto escalar $\langle,\rangle$ en $Alt^p(V)$ tal que
$$\langle w_1\land\dots\land w_p,\tau_1\land\dots\land\tau_p\rangle=\det(\langle w_i,\tau_j\rangle)$$
para cualesquiera $w_i,\tau_j\in Alt^1(V)$, y 
$$\langle w,\tau\rangle=i^{-1}(w)\cdot i^{-1}(\tau).$$
Sea $\{b_1,\dots, b_n\}$ una base ortonormal de $V$ y sean $\beta_j=i(b_j)$. Probar que
$$\{\beta_{\sigma(1)}\land\dots\land\beta_{\sigma(p)}\mid \sigma\in S(p,n-p)\}$$
es una base ortonormal de $Alt^p(V)$.
\end{ejercicio}
\begin{solucion}
Vamos a probar que la expresión del enunciado determina un producto escalar comprobando que verifica los axiomas. Tengamos en cuenta que por la definición de $i$ y del producto escalar del ejercicio anterior, $i^{-1}(w)\cdot i^{-1}(\tau)=i(i^{-1}(w))(i^{-1}(\tau))=w(i^{-1}(\tau))=\tau(i^{-1}(w))$ por simetría del producto escalar. Entonces, $\det(w_i(i^{-1}(\tau_j))=w_1\land\dots\land w_p(i^{-1}(\tau_1),\dots, i^{-1}(\tau_p))$ o también $\det(\tau_j(i^{-1}(w_i))=\tau_1\land\dots\land\tau_p(i^{-1}(w_1),\dots, i^{-1}(w_p))$. Así, $\langle w+w',v\rangle$ sería simplemente evaluar $w+w'$ sobre los $i^{-1}(v_i)$, por lo que claramente es multilineal. 

Dado $w\in Alt^`p$, $w=\sum_{\sigma\in S(p,n-p)}w_{\sigma}\varepsilon_{\sigma}$. Veamos que la aplicación es semidefinida positiva. Usamos la notación $e_i$ y $\varepsilon_i$ para la base ortonormal y su dual respectivamente. Sean $\sigma,\sigma'\in S(p,n-p)$.
\begin{gather*}
\langle \varepsilon_{\sigma}, \varepsilon_{\sigma'}\rangle=\langle \varepsilon_{\sigma(1)}\land\dots\varepsilon_{\sigma(p)},\varepsilon_{\sigma'(1)}\land\dots\varepsilon_{\sigma'(p)}\rangle=\\
\varepsilon_{\sigma(1)}\land\dots\varepsilon_{\sigma(p)}(i^{-1}(\varepsilon_{\sigma'(1)}),\dots, i^{-1}(\varepsilon_{\sigma'(p)}))=\\
\varepsilon_{\sigma(1)}\land\dots\land\varepsilon_{\sigma(p)}(e_{\sigma'(1)}, \dots, e_{\sigma'(p)})
\end{gather*}
Esto vale 0 si $\sigma\neq\sigma'$ y 1 si $\sigma=\sigma'$, por lo que se tiene el resultado. 
Además es simétrica porque el determinante de una matriz coincide con el de su traspuesta.


\end{solucion}

\newpage

\begin{ejercicio}{2.6}
Supongamos que $w\in Alt^p(V)$. Let $v_1,\dots, v_p$ vectores de $V$ y sea $A=(a_{ij})$ una matriz $p\times p$. Probar que para $w_i=\sum_{j=1}^pa_{ij}v_j$ ($1\leq i\leq p$) se tiene
$$w(w_1,\dots, w_p)=\det(A)w(v_1,\dots, v_p).$$
\end{ejercicio}
\begin{solucion}
Aplicar la multilinealidad junto con la definición de alternado, incluyendo que cambia el signo al trasponer.
\[
w=\sum_{\sigma\in S(p,n-p)} w_{\sigma}\varepsilon_{\sigma}\Rightarrow w(w_1,\dots, w_p)=\sum_{\sigma}\varepsilon_{\sigma}(w_1,\dots, w_p)=\det A\sum_{\sigma}(v_1,\dots, v_p). 
\]
Veámoslo.
\[
\varepsilon_{\sigma}(w_1,\dots, w_p)=\sum_{\tau\in S(p)}\prod_{k=1}^p a_{k,\tau(k)} \varepsilon_{\sigma}(v_{\tau(1)},\dots, v_{\tau(p}))=\sum_{\tau\in S(p)}sgn(\tau)\prod_{k=1}^p a_{k,\tau(k)} \varepsilon_{\sigma} (v_{1},\dots, v_{p}))=
\]
\[
\det(A)\varepsilon_{\sigma}(v_1,\dots, v_p)
\]
\end{solucion}

\newpage

\begin{ejercicio}{2.7}
Probar para $f:V\to W$ que

$$Alt^{p+q}(f)(w_1\land w_2)=Alt^p(f)(w_1)\land Alt^q(f)(w_2),$$
donde $w_1\in Alt^p(W)$ y $w_2\in Alt^q(W)$.
\end{ejercicio}
\begin{solucion}
Sean $v_1,\dots, v_{p+q}\in W$. 
\begin{gather*}
Alt^{p+q}(f)(w_1\land w_2)(v_1,\dots, v_{p+q})=w_1\land w_2(f(v_1),\dots, f(v_{p+q})=\\
\sum_{\sigma}sgn(\sigma)w_1(f(v_{\sigma(1)}),\dots, f(v_{\sigma(p)}))w_2(f(v_{\sigma(p+1)}),f(v_{\sigma(p+q}))=\\
\sum_{\sigma}sgn(\sigma) Alt^p(f)(w_1)(v_{\sigma(1)},\dots, v_{\sigma(p)})Alt^q(f)(w_2)(v_{\sigma(p+1)},\dots, v_{\sigma(p+q)})=\\
Alt^p(f)(w_1)\land Alt^q(f)(w_2)(v_1,\dots, v_{p+1})
\end{gather*}
\end{solucion}

\begin{ejercicio}{2.9}
Sea $V$ un espacio vectorial $n$-dimensional con producto escalar $\langle,\rangle$. Por el ejercicio \ref{ejer:2.5} obtenemos un producto escalar en $Alt^p(V)$ para todo $p$, en particular en $Alt^n(V)$. 

Un \emph{elemento de volumen} de $V$ es un vector unitario $vol\in Alt^n(V)$. El operador \emph{estrella de Hodge}
\[
*: Alt^p(V)\to Alt^{n-p}(V)
\]  
está definido por la ecuación $\langle *w,\tau\rangle vol=w\land \tau$. Probar que $*$ está bien definido y es lineal.

Sea $\{e_1,\dots, e_n\}$ una base ortonormal de $V$ con $vol(e_1,\dots, e_n)=1$ y $\{\varepsilon_1,\dots, \varepsilon_n\}$ la base ortonormal dual de $Alt^1(V)$. Probar que 
\[
*(\varepsilon_1\land\dots\land\varepsilon_p)=\varepsilon_{p+1}\land\dots\land\varepsilon_n
\]
y en general que
\[
*(\varepsilon_{\sigma(1)}\land\dots\land\varepsilon_{\sigma(p)})=\varepsilon_{\sigma(p+1)}\land\dots\land\varepsilon_{\sigma(n)}
\]
con $\sigma\in S(p,n-p)$. Probar que $*\circ *=(-1)^{n(n-p)}$ en $Alt^p(V)$.
\end{ejercicio}
\begin{solucion}
PROBAR QUE EXISTE VOLUMEN Y VER SI ES ÚNICO Y CÓMO VARÍA AL CAMBIAR DE BASE ORTONORMAL
\end{solucion}

\newpage

\begin{ejercicio}{2.10}
Sea $V$ un espacio vectorial de dimensión 4 y $\{\varepsilon_1,\dots,\varepsilon_4\}$ una base de $Alt^1(V)$. Sea $A$ una matriz antisimétrica y definamos
\[
\alpha=\sum_{i<j}a_{ij}\varepsilon_i\land\varepsilon_j.
\]
Probar que $\alpha\land\alpha=0$ si y solo si $\det(A)=0$.  Digamos que $\alpha\land\alpha=\lambda\varepsilon_1\land\varepsilon_2\land\varepsilon_3\land\varepsilon_4$. ¿Cuál es la relación entre $\lambda$ y $\det(A)$?
\end{ejercicio}
\begin{solucion}
\begin{gather*}
\alpha\land\alpha=(\sum_{i<j}a_{ij}\varepsilon_i\land\varepsilon_j)\land (\sum_{i<j}a_{ij}\varepsilon_i\land\varepsilon_j)=\\
(2a_{12}a_{34}-2a_{13}a_{24}+2a_{14}a_{23})(\varepsilon_1\land\varepsilon_2\land\varepsilon_3\land\varepsilon_4)
\end{gather*}
Esto es justamente $\frac{Pf(A)}{4}(\varepsilon_1\land\varepsilon_2\land\varepsilon_3\land\varepsilon_4)$, donde $Pf(A)$ es el pfaffiano\footnote{https://en.wikipedia.org/wiki/Pfaffian} de $A$. Como $\det(A)=Pf(A)^2$ se tiene el resultado. Si el módulo de $\alpha\land\alpha$ fuera $\lambda$, entonces $\lambda=Pf(A)/4=\sqrt{\det(A)}/4$. 
\end{solucion}

\end{document}