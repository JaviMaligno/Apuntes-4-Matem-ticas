\documentclass[twoside]{article}
\usepackage{../../estilo-ejercicios}
\newcommand{\media}[1]{{\overline{#1}}}
\newcommand{\muestra}[1]{{\underline{#1}}}
\newcommand{\m}[1]{{\muestra{#1}}}
\newcommand{\mX}{{\muestra{X}}}

%--------------------------------------------------------
\begin{document}

\title{Ejercicios de Análisis de Datos Multivariantes}
\author{Diego Pedraza López, Javier Aguilar Martín, Rafael González López}
\maketitle

\section{Análisis de Componentes Principales}
Sea $X$ una matriz de datos $n \times p$ y $\widehat{\Sigma}$ la matriz de varianzas asociada.

\begin{enumerate}
\item Describe cual es el objetivo fundamental del ACP.

Determinar un espacio de dimensión "reducida" que represente adecuadamente un conjunto de n observaciones $p$-−dimensionales. Para ello sustituimos las variables originales por un número pequeño de combinaciones lineales de las variables originales, incorreladas y "perdiendo" poca información.

\item Define las puntuaciones de los $n$ puntos sobre las $p$ CP. 
\item Demuestra que $d(\muestra{x_i}, \muestra{x_j}) = d(\muestra{y_i}, \muestra{y_j})$ con $\muestra{y_i}$ el vector de puntuaciones sobre las componentes principales correspondiente al individuo $i$. 

Sabemos que $\muestra{y_i} = \hat{E}'\muestra{x_i}$ con $E$ una matriz ortogonal, luego se tiene de manera inmediata que 
$$
d^2(\muestra{y_i},\muestra{y_j})=(\muestra{y_j}-\muestra{y_i})'(\muestra{y_j}-\muestra{y_i}) = (\muestra{x_j}-\muestra{x_i})\hat{E}\hat{E}'(\muestra{x_j}-\muestra{x_i}) = (\muestra{x_j}-\muestra{x_i}) '(\muestra{x_j}-\muestra{x_i}) = d^2(\muestra{x_i},\muestra{x_j})
$$
\item Demuestra que la varianza de la $j$−ésima CP es el $j$−ésimo mayor autovalor de $\widehat{\Sigma}$.

Sabemos que $$\Sigma_{\muestra{Y}} = Cov(\hat{E}'\muestra{X})= \hat{E}'Cov(\muestra{X})\hat{E} =\hat{E}' \hat{\Sigma}\hat{E}= \hat{\Lambda}$$
La varianza de la $j$-ésima componente principal es el $j$-ésimo elemento diagonal de $\hat{\Lambda}$, que es precisamente el $j$-ésimo mayor autovalor de $\hat{\Sigma}$.

\item Determina el coeficiente de correlación lineal entre $\muestra{y}_{{(i)}}$ ($i$−ésima CP) y $\muestra{x}_{(j)}$ .($j$−ésima variable).

Tengamos en cuenta que $\hat{e_i}'\hat{\Sigma}_{\cdot j} = (\hat{E}')_{i \cdot}\hat{\Sigma}_{\cdot j}=(\hat{E}'\hat{\Sigma})_{ij}=(\hat{\Lambda}\hat{E}')_{ij}=\hat{\lambda}_i \hat{e}_{ij}$. Entonces:
$$
Cov(\muestra{y}_{{(i)}},\muestra{x}_{{(j)}}) = Cov(\hat{e_i}' \muestra{x},\muestra{x}_{{(j)}}) = \hat{e_i}'Cov(\muestra{x},\muestra{x}_{(j)})= \hat{e_i}'\hat{\Sigma}_{\cdot j} = \hat{\lambda}_i \hat{e}_{ij}
$$ 
Por tanto, tenemos
$$
r(\muestra{y}_{{(i)}},\muestra{x}_{{(j)}}) = \frac{\hat{\lambda}_i \hat{e}_{ij}}{\hat{\sigma}_{j}\sqrt{\hat{\lambda_j}}} = \frac{\hat{e}_{ij}\sqrt{\hat{\lambda}_i}}{\hat{\sigma}_j}
$$
\item Demuestra que la primera CP es la combinación lineal (normalizada) de máxima varianza. 

Sea $H=\{t\in \R^p \mid t't=1\}$. Queremos demostrar que
$$
\hat{\sigma}_{\muestra{y}_{(1)}}=\sup_{t\in H}\hat{\sigma}^2_{t'\muestra{x}}
$$
Como las columnas de $\hat{E}$ forman una base (ortonorma) de $\R^p$, para cada $t$ existe $c\in \R^p$ tal que $t = \sum_{i=1}^p c_i \hat{e}_i$. Es decir, $t=\hat{E}c$. Además, $c'c = t'EE't=t't=1$ y
$$
\hat{\sigma}^2_{t'\muestra{x}} = t'\hat{\Sigma}t =  c'\hat{E}'\hat{\Sigma}\hat{E}c = c'\hat{\Lambda}c = \sum_{i=1}^p \hat{\lambda}_i c_i^2 \leq \hat{\lambda}_1 \sum_{i=1}^p  c_i^2 = \hat{\lambda}_1 = \hat{\sigma}^2_{\muestra{y}_{(1)}}
$$
Por tanto,
$$
d
$$
\item Demuestra que la suma de las varianzas de las variables originales es igual a la suma de las varianzas de las CP.

Sabemos que $trace(AB)=trace(BA)$, luego
$$
trace(\hat{\Sigma})=trace(\hat{E}\hat{\Lambda}\hat{E}')=trace((\hat{E}\hat{\Lambda})\hat{E}') = trace(\hat{E}'(\hat{E}\hat{\Lambda})) = trace(\hat{\Lambda})=\sum_{i=1}^p \hat{\lambda}_i
$$
\item Demuestra que las CP están incorreladas.

Es trivial, pues la matriz de varianzas covarianzas es diagonal, luego las correlaciones entre distintas variables son nulas.
\item Considérese el conjunto de datos muestrales.
\[ \begin{pmatrix}5 & 3 & ? & ? & ?\\4 & a & ? & ? & ?\\4 & a & ? & ? & ?\\1 & a & ? & ? & ?\end{pmatrix}\]
\begin{enumerate}
	\item Determina el mayor autovalor de $\widehat{\Sigma}$ sabiendo que el autovector asociado es $(1,0,0,0,0)$.
	
	Claramente el autovalor asociado será el elemento $\hat{\Sigma}_{11}$. Si util
$$
\hat{\sigma}_{11}=\hat{\sigma}_1^2=\frac{1}{3}\sum_{i=1}^4(X_{i1}-\frac{5+4+4+1}{4})^2 = \frac{1}{3}\left[(5-\frac{14}{4})^2 +  2(4-\frac{14}{4})^2 + (1-\frac{14}{4})^2\right]= 3
$$

	\item Calcula el valor de $a$ sabiendo que el segundo autovector es $(0,1,0,0,0)$. 
	
Sabemos que la $\hat{\sigma}_{\muestra{y}_{(1)}\muestra{y}_{(2)}}=0$. Tenemos
$$
\hat{\sigma}_{\muestra{y}_{(1)},\muestra{y}_{(2)}} = \hat{\sigma}_{\hat{e}_1'\muestra{x},\hat{e}_2'\muestra{x}} = \hat{e}_1'\Sigma \hat{e}_2 = \hat{\sigma}_{12}
$$
Haciendo el cálculo de dicho estimador obtenemos
\begin{align*}
\hat{\sigma}_{12}&=\frac{1}{3}\sum_{i=1}^4(X_{i1}-\frac{14}{4})(X_{i2}-\frac{3+3a}{4})\\
&=\frac{1}{3}\left[(5-\frac{14}{4})(3-\frac{3+3a}{4})+2(4-\frac{14}{4})(a-\frac{3+3a}{4})+(1-\frac{14}{4})(a-\frac{3+3a}{4})\right]\\
&=\frac{3-a}{2}
\end{align*}
Por tanto $a=3$.

\end{enumerate}
\end{enumerate}
\newpage
\section{Análisis Factorial}
\begin{enumerate}
\item Describe cual es el objetivo básico del Análisis Factorial.

El objetivo del Análisis Factorial (AF) es describir, si es posible, la estructura de covarianzas entre diversas variables en términos de un número reducido de variables
latentes, no observables, denominadas factores.

Supuesto que las variables están correladas, el objetivo es reducir la redundancia usando un menor número de factores

\item Describe el Modelo Factorial Ortogonal y las hipótesis del mismo.

Sea $\muestra{X}$ un vector aleatorio (observable) p− dimensional, $X \sim (\mu,\Sigma)$. El modelo factorial postula que el vector aleatorio $\muestra{X}$ depende linealmente de "un número reducido" de variables aleatorias no observables $F_1,\dotsc,F_m$, denominadas factores comunes y $p$ fuentes de variación adicionales $\varepsilon_1, ..., ε_p$ denominados errores aleatorios o
factores específicos:
$$
X-\mu = LF+\varepsilon
$$
La matriz $L$ es la matriz de cargas factoriales. $l_{ij}$ es la carga factorial de la variable $i$-ésima sobre el $j$-ésimo factor. Los factores comunes pueden estar relacionados con todas las variables originales, mientras que cada factor específico está relacionado con una única variable original. Tanto los factores comunes como los factores específicos son no observables. 

Además, asumimos $\E[F]=\E[\varepsilon]=0$. $Cov[F]=I_m$, $Cov(\varepsilon)=\Psi$ matriz diagonal y $Cov(F,\varepsilon)=0$.
\item Demuestra que bajo las hipótesis del modelo:
\begin{gather}
\Sigma = L L' + \Psi \Leftrightarrow
\begin{cases}
\sigma_{ii} = l_{i1}^2 + \dots + l_{im}^2 + \psi_i\\
\sigma_{ik} = \sum_{j=1}^m l_{ij} \cdot l_{kj}
\end{cases}\\
Cov(\m{X}, \m{F}) = L \Leftrightarrow l_{ij} = Cov(X_i, F_j)
\end{gather}

La doble implicación es clara, así que demostramos la igualdad izquierda.
$$
\Sigma = Cov(X-\mu) = Cov(LF+\varepsilon) = LCov(F)L' + Cov(\varepsilon) = LL'+\Psi
$$
Para la segunda igualdad
$$
Cov(X,F) = Cov(X-\mu,F) = Cov(LF+\varepsilon,F) = Cov(LF,F)+Cov(\varepsilon,F) = LI_m + 0 = L
$$
\item Demuestra que si las variables originales están normalizadas

\[ \rho(X_i, F_j) = l_{ij}. \]

Si las variables están estandarizadas $\hat{\sigma}{X_i}=1$. Además, $Cov(F_j) = 1$ por hipótesis del modelo. Por tanto
$$
\rho(X_i,F_j) = \frac{Cov(X_i,F_j)}{\sigma_{X_i}\sigma_{F_j}} = L_{ij} = l_{ij}
$$
\item Describe las (3) etapas básicas, y el objetivo de cada una de ellas, cuando se realiza un análisis factorial.
\item Describe el Método de las Componentes Principales para estimar la matriz de cargas factoriales y la matriz de varianzas específicas.
\item Demuestra que en el Método de las Componentes Principales, las cargas factoriales estimadas no cambian cuando se incrementa el número de factores.
\item Describe
\begin{enumerate}
	\item En qué consiste la rotación de factores.
	\item En que consiste la rotación varimax y qué pretende.
\end{enumerate}
\end{enumerate}

\newpage
\section{Análisis de Correspondencia}
\begin{enumerate}
\item 
\begin{enumerate}
\item Plantea los problemas de independencia de caracteres y de homgeneidad de poblaciones.
\item Demuestra que los estadísticos $\chi^2$ asociados a ambos problemas y sus distribuciones coinciden.
\end{enumerate}
\item Sean $r_1', \dots, r_n'$ las distribuciones condicionadas por filas asociadas a un tabla de contingencia
\[\begin{matrix}
    & B_1    & \dots & B_j    & \dots & B_p\\
A_1 & N_{11} & \dots & N_{1j} & \dots & N_{1p} & N_{1.}\\
\vdots & \vdots & & \vdots & & \vdots & \vdots\\
A_i & N_{i1} & \dots & N_{ij} & \dots & N_{ip} & N_{i.} = \sum_{j=1}^p N_{ij}\\
\vdots & \vdots & & \vdots & & \vdots & \vdots\\
A_n & N_{n1} & \dots & N_{nj} & \dots & N_{np} & N_{n.}\\
    & N_{.1} & \dots & N_{.j} = \sum_{i=1}^n N_{ij} & \dots & N_{.p} & N
\end{matrix}\]
\begin{enumerate}
	\item Define el centro de gravedad de las filas: $m_r'$
	\item Define la distancia ji-cuadrado entre dos distribuciones. 
	\item Demuestra que $\chi^2 = N\sum_{i=1}^n f_{i.} d_{\chi^2} (r_i, m_r)$
\end{enumerate}
\item Describe el objetivo del análisis de correspondencias (por filas).
\item Analogías y diferencias entre el ACP y el AC.
\item Plantea y resuelve el problema de optimización al que conduce el AC. 
\item Demuestra que el cuadrado de la distancia euclídea entre las proyecciones de dos filas $P_{r_i}$ y $P_{r_i'}$ coincide con la distancia \emph{ji−cuadrado} entre las filas $r_i$ y $r_i'$.
\item Concepto de vértice en el problema del AC por filas. Interpretación. 
\item Determina las coordenadas de $m_r'$ en la nueva base. 
\item Demuestra que $(0, m_r)$ es un autovalor-vector de la matriz
\[ S_r = \sum_{i=1}^n f_{i.} (r_i - m_r) (r_i - m_r)^t D_p^{-1} \]
\item Determina las coordenadas de las filas correspondientes al autovector $m_r$. ¿Qué evidencian?
\item Demuestra que la varianza ponderada de las coordenadas correspondientes al autovector $u_k$ se puede expresar como
\[ \sum_{i=1}^n f_{i.} (r_i^t D_p^{-1} u_k)^2 \]
\item Define la inercia total y demuestra que es igual a la suma de los autovalores de $S_r$.
\end{enumerate}

\section{Análisis Discriminante}
\begin{enumerate}
\item Discriminación en dos poblaciones con distribuciones conocidas
\begin{enumerate}
	\item Define la probabilidad total de error de clasificación de una regla discriminante.
	\item Determina la regla discriminante que minimiza la probabilidad total de error de clasificación 
	\item Plantea y resuelve el problema original de Fisher. 
	\item Demuestra que supuesto que las poblaciones se distribuyen según $N(\mu_i, \Sigma)$, $i = 1, 2$, la regla discriminante que minimiza la proba- bilidad total de error coincide con la resultante de la propuesta de Fisher.
\end{enumerate}
\item Coordenadas discriminantes canónicas 
\begin{enumerate}
	\item Planteamiento y solución.
	\item Describe el criterio de clasificación basado en las coordenadas canónicas.
\end{enumerate}
\end{enumerate}
\end{document}
