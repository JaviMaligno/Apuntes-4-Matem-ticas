
#############
Introducción
#############

Presentamos el método tree-based. Puede utilizarse tanto para problemas de regresión y clasificación.
Para ello "estratificaremos" o "segmentaremos" el predictor space en un cierto número de regiones.
Dado que el conjunto de las reglas de división que usamos para segmentar el predictor space pueden
resumirse en un árbol, estas aproximaciones se conocen como métodos de árbol de decisión.

# Foto árbol

En cuanto a la capacidad del método para ser interpretado, se considera que útil, pero en otros aspectos
como la precisión en las predicciones se encuentra ampliamente superado por otros métodos como el
aprendizaje supervisado.

Las regiones en las que finalmente quedan divido el predictor space se denominan "hojas" o "nodos finales".
Típicamente el árbol (figura) se dibuja con las hojas en la parte inferior. Los puntos del árbol donde 
se divide el predictor space se denominan "nodos internos" mientras que los segmentos que conectan los nodos
se denominan "ramas".

# Ejemplo Hitters

###########
Predicción 
###########

Sean X_1,...,X_p v.a. Para construir un arbol de regresion, particionamos el predictor space
en regiones R_1,...,R_J. Toda observación que cae en R_j, se le asocia la misma predicción, 
que es el valor medio de los valores de las observaciones de entrenamiento que se encuentra en R_j.

############################
Recursive binary splitting 
############################

Realizamos una "separación binaria recursiva" del espacio. Esto consiste en elegir un X_j y un punto de 
corte s y crear dos regiones  R_1 y R_2, donde X_j < s y X_j ≥ s respectivamente, de forma que se minimiza la ecuación

\[ \sum_{i\colon x_i \in R_1(j,s)} (y_i - \widehat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)} (y_i - ŷ_{R_2})^2 \]

donde ŷ_{R_j} es la y media de las observaciones de entrenamiento en la caja R_j. A continuación, en vez de 
particionar el espacio predictor sobre otra X_i,  particionamos una de las dos regiones que hemos generado 
antes. Repetimos el proceso recursivamente hasta que hemos creado suficientes cajas.

Un criterio para detener el algoritmo puede ser que en cada caja haya menos de un determinado número de elementos.

#########
Prunning
#########

Otra estrategia para generar un árbol es partir de un árbol "grande" T0 y "podarlo" para obtener otro árbol. ¿Cómo
determinamos la forma óptima de "podar" un árbol? Intuitivamente nuestro objetivo es minimizar la tasa de error,
lo cuál si realizamos por cross-validation puede ser muy engorroso, debido a que la cantidad de subárboles de un
árbol dado puede ser muy grande. La solución que damos a este problema es que solo buscaremos la optimalidad en un
conjunto pequeño de subárboles. 

# Algoritmo (comentarios a lo que viene en la presentación)

1. Por ejemplo, utilizando separación binaria recursiva".
2. Weakest Link Pruning / Cost complexity pruning. Indexamos los árboles según un parámetro alfa
   |T| es el número de hojas. Rm indexan los rectángulos que se corresponden con dichas hojas. Si
   alpha = 0 entonces T=T0. Si alfa crece, hay un "coste" por tener un gran número de nodos finales
3. 
