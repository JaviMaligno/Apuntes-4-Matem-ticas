
#############
Introducción
#############

Presentamos el método tree-based. Puede utilizarse tanto para problemas de regresión y clasificación.
Para ello "estratificaremos" o "segmentaremos" el predictor space en un cierto número de regiones.
Dado que el conjunto de las reglas de división que usamos para segmentar el predictor space pueden
resumirse en un árbol, estas aproximaciones se conocen como métodos de árbol de decisión.

# Foto árbol

En cuanto a la capacidad del método para ser interpretado, se considera que útil, pero en otros aspectos
como la precisión en las predicciones se encuentra ampliamente superado por otros métodos como el
aprendizaje supervisado.

Las regiones en las que finalmente quedan divido el predictor space se denominan "hojas" o "nodos finales".
Típicamente el árbol (figura) se dibuja con las hojas en la parte inferior. Los puntos del árbol donde 
se divide el predictor space se denominan "nodos internos" mientras que los segmentos que conectan los nodos
se denominan "ramas".

# Ejemplo Moratality Rates


######################
Árboles de regresión
######################

Sean X_1,...,X_p v.a. Para construir un arbol de regresion, particionamos el espacio de predicción
en regiones R_1,...,R_J. Toda observación que cae en R_j, se le asocia la misma predicción, 
que es el valor medio de los valores de las observaciones de entrenamiento que se encuentra en R_j.

############################
Recursive binary splitting 
############################

Realizamos una "separación binaria recursiva" del espacio. Esto consiste en elegir un X_j y un punto de 
corte s y crear dos regiones  R_1 y R_2, donde X_j < s y X_j ≥ s respectivamente, de forma que se minimiza la RSS
(suma de los residuos al cuadrado)

\[ \sum_{i\colon x_i \in R_1(j,s)} (y_i - \widehat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)} (y_i - ŷ_{R_2})^2 \]

donde ŷ_{R_j} es la y media de las observaciones de entrenamiento en la caja R_j. A continuación, en vez de 
particionar el espacio predictor sobre otra X_i,  particionamos una de las dos regiones que hemos generado 
antes. Repetimos el proceso recursivamente hasta que hemos creado suficientes cajas.

# Escribir fórmula general del RSS

Un criterio para detener el algoritmo puede ser que en cada caja haya menos de un determinado número de elementos.

#########
Prunning
#########

Otra estrategia para generar un árbol es partir de un árbol "grande" T0 y "podarlo" para obtener otro árbol. ¿Cómo
determinamos la forma óptima de "podar" un árbol? Intuitivamente nuestro objetivo es minimizar la tasa de error,
lo cuál si realizamos por cross-validation puede ser muy engorroso, debido a que la cantidad de subárboles de un
árbol dado puede ser muy grande. La solución que damos a este problema es que solo buscaremos la optimalidad en un
conjunto pequeño de subárboles. 

# Algoritmo (comentarios a lo que viene en la presentación)

1. Por ejemplo, utilizando separación binaria recursiva".
2. Weakest Link Pruning / Cost complexity pruning. Indexamos los árboles según un parámetro alfa
   |T| es el número de hojas. Rm indexan los rectángulos que se corresponden con dichas hojas. Si
   alpha = 0 entonces T=T0. Si alfa crece, hay un "coste" por tener un gran número de nodos finales.
3. Dividimos los datos de entrenamiento en K folds y repetimos los pasos anteriores usando todos los datos
   salvo el k-ésimo fold. Evaluamos el error cuadrático medio de la predicción como función de alfa. 
   Tomamos como "error" para cada alfa como la media de los errores calculados para el k-ésimo fold.
4. Volvemos al árbol T0 original y tomamos un subárbol que corresponda al valor obtenido de alfa.

# ¿Sería posible ver un ejemplo de esto?

# Tabla Tree Size / Mean Squared Error  



#############
Clasificacón
#############

La idea detrás de los árboles de clasificación es similar a la idea que hay tras los árboles de predicción, pero
en este caso buscamos una respuesta más "cualitativa" que "cuantitativa". Recuerdo que para la predicción en los 
árboles de regresión utilizamos la media de las observaciones que pertenecen a la misma hoja. Para los árboles de
clasificación tomaremos como predicción la moda de las observaciones que pertenecen a la misma hoja.

Naturalmente para variables cualitativas el RSS puede no tener mucho sentido como medida del error que cometemos.
La alternativa natural es el ratio del error de clasificiación (classification error rate). Esto es, la fracción
de observaciones que en cada región no pertenecen a la clase más común

\[ E = 1 - \max(\hat{p}_{mk}) \]

donde \hat{p}_{mk} es la proporción de observaciones de la m-ésiam región (hoja) de la clase k.
Esta medida de error no es lo suficientemente sensible cuando los árboles crecen y en la práctica, con el fin de
evaluar la calidad de una división del árbol, se suelen utilizar otras dos medidas

- Índice de Gini: Es una medida de la varianza total sobre las K clases. 
\[ G = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})  \]


- Entropía cruzada: Es una alternativa al índice de Gini. En ambos casos también se acerca a 0 cuando el nodo es puro.
\[ D = -\sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk} \]

Las tres medidas pueden utilizarse para la poda del árbol, pero la priemera es preferible si nuestro objetivo es 
precisión al a hora de predecir. Naturalmente, tanto la construcción como la poda del árbol se realiza análogamente
a las que hemos mostrado anteriormente.


###########################
Árboles vs Modelos lineales
###########################

A continuación oferecemos una comparativa entre lLos modelos lineales (que han sido / serán presentados por otros
compañeros). ¿Qué modelo es mejor? Pues depende de cómo sea el problema. Si los datos, por sus características,
se adecuan bien al modelo lineal, entonces las aproximiaciones y regresiones lineales funcionarán mejor, generalmente,
que los árboles de regresión. Si, en cambio, los datos tienen una estructura no lineal y compleja, entonces el modelo
de árboles se adaptarán mejor que los método clásicos.

# Algo como la figura 8.7 me parece un ejemplo magnífico de lo que se ha explicado arriba. 


#######################
Ventajas y desventajas
#######################

Ventajas:
- Son fáciles de explicar incluso si no saben nada de regresión lineal.
- Mucha gente piensa que son más parecidos a cómo funciona el sistema de decisión de los 
  seres humanos, a diferencia de otros modelos.
- Pueden ser presentados gráficamente y fácilmente interpretrados.
- Pueden incluir variables cualitativas sin problemas y sin necesida de crear nuevas variables.

Desventajas:
- No tienen tanta capacidad para predecir, en general, como otros modelos de regresión y clasificación.


#################################
Bagging, Random Forest, Boosting
#################################

A continuación preentamos tres modelos que usan a su vez árboles como "bloques" para construir modelos de predicción
más potentes.

########
Bagging
########

Comencemos por una primera observación. Sabemos que si tenemos n observaciones independientes Xi con varianza sigma^2, 
entonces la media muestral tiene varianza sigma^2/n. En otras palabras, hacer la media de las observaciones reduce la 
varianza. Por tanto, una manera natural de aumentar la precisión de nuestras prediccioens será tomar varios conjuntos
de entrenamiento de una población, construir modelos de predicción de manera separada con cada uno de ellos y, finalmente,
hacer la media de los resultados predichos. 

Este método puede no ser muy práctica pues en la vida real muchas veces no se tiene acceso a múltiples conjuntos de
entrenamiento. En lugar de eso, podemos toamr muestras aleatorias de un solo conjunto de entramiento y repetir el proceso.
A este último método es al que se le conoce como bagging. En la práctica, construimos B árboles de decisión usando B conjuntos
de entrenamiento obtenidos como acabamos de explicar y calculamos la media de las predicciones. Estos árboles son profundos
y no han sido podados, luego tienen poco sesgo pero una gran varianza. Haciendo la media, mantenemos el caracter del sesgo 
y además reducimos la varianza. Esta técnica ha demostrado ser muy efectiva en cuanto a precisión.

En el caso en el que tengamos variables cualitativas, la aproximación más simple consiste en "guardar" la clase predicha
por cada árbol y tomar el "voto de la mayoría", es decir, dar como respuesta la clase que más veces guardado.

##############
Random Forests
##############

Al igual que en Bagging, la mejora Random Forest considera
una colección de árboles, pero tratamos de que sean más incorrelados.
Para hacemos que cada árbol se forme sin usar todos los predictores.

Las predicciones se hacen de la misma forma que en Bagging.
Este método mejora la sensibilidad de las predicciones,
especialmente cuando hay un predictor más fuerte que el resto.

########
Boosting
########

Boosting es también similar a Bagging, pero en lugar de crear
los árboles independientemente, formamos los árboles usando
información de los árboles anteriores.

En concreto, empezamos con un árbol normal y luego construimos
árboles que modelicen los residuos del árbol anterior.
El árbol final será una suma de los árboles con un factor de encojimiento
λ.

Este factor λ provoca un ralentizamiento en el aprendizaje, a cambio
de un mejor resultado. 