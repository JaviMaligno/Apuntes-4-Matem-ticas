\documentclass[PM.tex]{subfiles}
\begin{document}
\chapter{Análisis convexo}
\section{Conjuntos convexos}

\begin{defi}
Un conjunto $S\subseteq\R^n$ es \textbf{convexo} si $\forall x^1,x^2\in S$ y $\forall  \lambda\in[0,1]$ se cumple que $\lambda x^1 + (1-\lambda)x^2\in S$.
\end{defi}

\begin{defi}
Un \textbf{semiespacio} es un conjunto de la forma $H^-=\left\{x\in\R^n: a'x\leq b, a\in\R^n, b\in\R\right\}$ o de la forma $H^+=\left\{x\in\R^n: a'x\geq b, a\in\R^n\,b\in\R\right\}$.
\end{defi}

\begin{prop}
$H^-$ es convexo.
\end{prop}

\begin{dem}
Sea $x(\lambda)\equiv \lambda x^1 + (1-\lambda)x^2, \lambda\in[0,1]$. Se tiene lo siguiente
\[
a'x(\lambda)=a'(\lambda x^1 + (1-\lambda)x^2)=\lambda a'x^1 +(1-\lambda)a'x^2\leq \lambda b+(1-\lambda)b=b
\]
Como queríamos demostrar. $\QED$
\end{dem}

\begin{prop}
Si $S_1$ y $S_2$ son convexos entonces $S_1\cap S_2$ es convexo
\end{prop}
\begin{dem}
Definimos $x(\lambda)$ igual que en la demostración anterior. Supongamos que $S_1$ y $S_2$ tienen intersección no vacía, pues en otro caso el resultado se sigue trivialmente. Dados $x^1,x^2\in S_1 \cap S_2$ se tiene por convexidad de $S_1$ que $x(\lambda)\in S_1$. Análogamente, $x(\lambda)\in S_2$. Por lo tanto, $x(\lambda)\in S_1\cap S_2$. $\QED$
\end{dem}

\begin{defi}
Un \textbf{poliedro} es el conjunto de puntos definido por la intersección de un número finito de semiespacios. Visto matricialmente, dados $A\in\R^{m\times n}, b\in \R^n$ se define el poliedro $P=\left\{x\in \R^n: Ax\leq b\right\}$. Si denotameos por $a_i$ a la i-ésima fila de $A$ vemos que este conjunto se reescribe como $\left\{x\in\R^n: a_ix\leq b_i\ \forall i=1,\dots, n\right\}$, que es equivalente a la primera definición. Un poliedro acotado se llama \textbf{politopo}. 
\end{defi}

\begin{coro}
Los poliedros son convexos.
\end{coro}

\begin{defi}
Se define el \textbf{rayo} por un punto $x\in\R^n$ como el conjunto $\left\{y\in\R^n: y=\lambda x, \lambda\geq 0\right\}$.
\end{defi}

\begin{defi}
Sea S convexo y $x\in S$. El punto $x$ dice \textbf{punto extremo} si $x=\lambda x^1 +(1-\lambda)x^2$ con $x^1,x^2\in S$ y $\lambda\in (0,1)$ implica que $x^1=x^2=x$.
\end{defi}
\begin{nota}  En el caso de los poliedros, los puntos extremos coinciden con la idea de vértice. En una bola euclídea cerrada su frontera es de puntos extremos.
\end{nota}

\begin{defi}
Diremos que $d\in\R^n$ es una \textbf{dirección} de $S\subseteq\R^n$ si $\forall x\in S$ se tiene que $x+\alpha d\in S\ \forall\alpha\geq 0$.
\end{defi}

\begin{defi} Una dirección $d$ es \textbf{extrema} si $d=\alpha^1d^1+\alpha^2d^2$, siendo $d^i$ direcciones de $S$ y $\alpha^1,\alpha^2>0$ implica que $d^1$ es proporcional a $d^2$.
\end{defi}

\begin{defi}
Dados $r$ puntos $x^1,\dots, x^r\in\R^n$ llamamos \textbf{combinación convexa} de estos puntos a $\sum_{i=1}^r\lambda_i x^i$ si $\lambda_i\geq 0\ \forall i=1,\dots, r$ y $\sum_{i=1}^r\lambda_i=1$.
\end{defi}

\section{Teorema de Carathéodory}
\begin{defi} Sean $a^1,\dots, a^d\in\R^n$ afinmente independientes (esto es, que fijado un punto, los vectores diferencia de este con los demás son linealmente independientes). Se denomina \textbf{símplex $(d-1)$-dimensional} a 
\[
S(d-1)= \left\{x\in\R^n:x=\sum_{i=1}^d\lambda_i a^i,\lambda_i\geq 0, \sum_{i=1}^d\lambda_i =1\right\}
\]
\end{defi}
\begin{example}
$S(1)$ es un segmento, $S(2)$ es un triángulo relleno y $S(3)$ es un tetraedro relleno.
\end{example}

\begin{defi} Dado un conjunto $S\subset\R^n$, su \textbf{envolvente} (envoltura) \textbf{convexa} $CO(S)$ es
\[
CO(S)=\left\{x\in\R^n: x=\sum_{i=1}^d\lambda_i x^i, r<+\infty, x^i\in S^1, i=1,\dots, r, \lambda_i\geq 0, \sum_{i=1}^d\lambda_i =1\right\}
\]
$CO(S)$ es el menor convexo que contiene a $S$.
\end{defi}

\begin{defi} Diremos que $C\subset\R^n$ es un cono si contiene a todos sus rayos, es decir, $\forall x\in C$ $\lambda x\in C, \lambda\geq 0$.
\end{defi}
\begin{example}
Una semirrecta que pasa por el origen es un cono. No todos los conos son conjuntos convexos.

\definecolor{zzttqq}{rgb}{0.6,0.2,0.}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\clip(-3,-2.5) rectangle (3,2.5);
\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (0.,0.) -- (2.,3.) -- (4.,3.) -- cycle;
\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (0.,0.) -- (-4.5,-3.) -- (-2.5,-3.) -- cycle;
\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (0.,0.) -- (4.5,-3.) -- (2.5,-3.) -- cycle;
\draw [color=zzttqq] (0.,0.)-- (2.,3.);
\draw [color=zzttqq] (2.,3.)-- (4.,3.);
\draw [color=zzttqq] (4.,3.)-- (0.,0.);
\draw [color=zzttqq] (0.,0.)-- (-4.5,-3.);
\draw [color=zzttqq] (-4.5,-3.)-- (-2.5,-3.);
\draw [color=zzttqq] (-2.5,-3.)-- (0.,0.);
\draw [color=zzttqq] (0.,0.)-- (4.5,-3.);
\draw [color=zzttqq] (4.5,-3.)-- (2.5,-3.);
\draw [color=zzttqq] (2.5,-3.)-- (0.,0.);
\draw (2.,2.)-- (2.,-2.);
\draw (0.,-4.126140413226097) -- (0.,4.020852846215659);
\draw [domain=-6.367650924229384:10.452891711961001] plot(\x,{(-0.-0.*\x)/2.8572768088505205});
\begin{scriptsize}
\draw [fill=black] (2.,2.) circle (2.5pt);
\draw [fill=black] (2.,-2.) circle (2.5pt);
\end{scriptsize}
\end{tikzpicture}
\end{example}\

\begin{theorem}[Teorema de Carathéodory]
Sea $S\subseteq\R^n$. Si $x\in CO(S)$ entonces:
\[ x = \sum_{i=1}^{n+1} λ_i x^i \]
con $x^i\in S, λ_i ≥ 0$ y $\sum_{i=1}^{n+1}λ_i=1$. Es decir, cualquier punto de la envoltura se puede expresar como combinación lineal convexa de a lo sumo $n+1$ puntos.
\end{theorem}
\begin{dem}
Sea $x \in CO(S)$ y supongamos que:
\[ x = \sum_{i=1}^r λ_i x^i \]
Si $r ≤ n+1$, el teorema está probado. Si $r > n+1$, consideramos los $r-1$ vectores linealmente dependientes $x²-x¹, x³-x¹, \dots, x^{r}-x¹$. Entonces existen $μ_2,\dots,μ_r$ no todos nulos tales que:\[ 0 = \sum_{i=2}^r μ_i (x^i-x¹) = \sum_{i=2}^r μ_i x^i - x^1\sum_{i=2}^r μ_i\]
Tomamos $μ_1 := -\sum_{i=2}^r μ_i$ de manera que se tiene que:
\[ \sum_{i=1}^r μ_i x^i = 0\]
\[ \sum_{i=1}^r μ_i = 0 \]
Sea $α \in \R$. Consideramos:
\[ x = x - α0 = \sum_{i=1}^r λ_i x^i - α \sum_{i=1}^r μ_i x^i = \sum_{i=1}^r (λ_i - α μ_i)x^i \]
Definimos $γ_i = λ_i - α μ_i$ y buscamos $α$ de manera que $γ_i x^i$ sea una combinación convexa. Tomando:
\[ α = \min_{1≤i≤r} \left\{ \frac{λ_i}{μ_i} : μ_i > 0\right\} = \frac{λ_{i*}}{μ_{i*}} \]
Entonces se tiene que $γ_i \geq 0$ y:
\[ \sum_{i=1}^r γ_i = \sum_{i=1}^r (λ_i -α μ_i) = \sum_{i=1}^r λ_i - α \sum_{i=1}^r μ_i = 1 - α \cdot 0 = 1\]
Sin embargo, $γ_{i*} = λ_{i*} - \frac{λ_{i*}}{μ_{i*}}μ_{i*} = 0$, luego $x$ es una combinación convexa de a lo sumo $r-1$ puntos de $S$. Podemos repetir esta construcción hasta llegar a $r = n+1$.
$\QED$
\end{dem}

Sea $A \subset \R^{m \times n}$, $rg(A) = m$, $m ≤ n$ y $b \in \R^m$. Consideramos $P = \{ x \in \R^n : Ax=b, x ≥ 0\}$ (poliedro en forma estándar).

\begin{theorem}[Teorema de caracterización de puntos extremos]\label{carac-extremos}
$x$ es un punto extremo de $P$ si y sólo si existe $B \subset A$ con $B \in \R^{m \times m}$, $rg(B)=m$, $B^{-1}b ≥ 0$ y $x = \begin{pmatrix} B^{-1}b\\0\end{pmatrix}$
\end{theorem}

\begin{dem}
($\Leftarrow$) Supongamos que $x =\begin{pmatrix} B^{-1}b\\0\end{pmatrix}$, $B \subset A$ con $B \in \R^{m \times m}$, $rg(B)=m$, $B^{-1}b ≥ 0$. Usaremos reducción al absurdo, supongamos que existe $x^1$, $x^2 \in P$ tal que $x = λx^1+(1-λ)x^2$ con $λ \in (0,1)$. Tenemos que $A = [B\ N]$ y $x = \begin{pmatrix}x_B\\x_N\end{pmatrix} = \begin{pmatrix} B^{-1}b\\0\end{pmatrix}$, $x^1 = \begin{pmatrix}x_B^1\\x_N^1\end{pmatrix}$ y $x^2 = \begin{pmatrix}x_B^2\\x_N^2\end{pmatrix}$ Luego:
\[ x = \begin{pmatrix} B^{-1}b\\0\end{pmatrix} = λ\begin{pmatrix}x_B^1\\x_N^1\end{pmatrix}+(1-λ)\begin{pmatrix}x_B^2\\x_N^2\end{pmatrix} \]
De ahí, $0 = λ x_N^1 + (1-λ) x_N^2$. Como $x_N^i ≥ 0$, se sigue que $x_N^i = 0$. De $x^i \in P$, se sigue que $Ax^i = b \equiv [B\ N]\begin{bmatrix}x_B^i\\0
\end{bmatrix} = b \equiv Bx_B^i + N0 = b$. Por lo tanto: $x_B^i = B^{-1}b \geq 0$, luego $x^i = \begin{pmatrix}B^{-1}b\\0\end{pmatrix}=x$ para $i=1,2$.

($\Rightarrow$) Supongamos que $x$ fuera un punto extremo de $P$. Se tiene que $Ax=b, X ≥ 0$. Como $rg(A) = m$ podemos suponer sin pérdida de generalidad que $x^1 =(x_1,\dots,x_k,0,\dots,0)$ con $x_i > 0$ y $k ≤ m$.
Vamos a probar que las columnas asociadas a estas coordenadas $(a_1, a_2, \dots, a_k)$ son linealmente independientes. Si no lo fueran, existirían $λ_1,λ_2, \dots, λ_k$ tales que $\sum_{i=1}^k λ_i a_i = 0$. Denotamos or $λ' =(λ_1,\dots,λ_k,0,\dots,0)$. Entonces $Aλ=λ_1a_1 + λ_2a_2 + \cdots + λ_ka_k + 0a_{k+1}+\cdots+0a_n = 0$. Entonces podemos considerar los puntos $x^1 = x+αλ$ y $x^2 = x-αλ$ y $α > 0$. Claramente $x^1 \neq x^2$ y $x = \frac{1}{2}x^1+ \frac{1}{2}x^2$. Veamos que $x^i \in P$ para $i=1,2$.
\[ Ax^i = A(x + αλ) = Ax + α(\underset{=0}{Aλ}) = Ax = b \]
\[ x^i = x \pm αλ = \begin{pmatrix}x_1\\\vdots\\x_k\\0\\\vdots\\0\end{pmatrix} + α \begin{pmatrix}λ_1\\\vdots\\λ_k\\0\\\vdots\\0\end{pmatrix} \]
Por hipótesis, $x_1,\dots,x_k \geq 0$. Tomando $α$ suficientemente pequeño, podemos asegurar que $x_i \pm αλ_i > 0$. Entonces $x^i \geq 0$ y $x^i \in P$. Esto contradice que $x$ sea un punto extremo. Podemos entonces asegurar que $(a_1,\dots,a_k)$ son linealmente independientes. Podemos completar $(a_1,\dots,a_m)$ hasta $m$ columnas linealmente independientes (podemos ya que $rg(A) = m$). Tomamos $B = (a_1,\dots,a_m)$. Entonces:
\[ Ax =[B\ N]\begin{pmatrix}x_1\\\vdots\\x_k\\0\\\vdots\\0\end{pmatrix}_{(n)} = B \begin{pmatrix}x_1\\\vdots\\x_k\\0\\\vdots\\0\end{pmatrix}_{(m)} +N0_{(m-n)} = b \]
Luego: \[ \begin{pmatrix}x_1\\\vdots\\x_k\\0\\\vdots\\0\end{pmatrix} = B^{-1}b \geq 0 \qquad x = \begin{pmatrix}B^{-1}b\\0\end{pmatrix} \]
\end{dem}

\begin{nota}
El número de puntos extremos de los poliedro es finito y está acotado por el número combinatorio $\begin{pmatrix}n\\m\end{pmatrix}$.
\end{nota}

\begin{lema}
$d$ es una dirección de $P$ si y sólo si $Ad=0$, $d ≥ 0$.
\end{lema}

\begin{dem}
$d$ es dirección sii $\forall x \in P$, $x + λd \in P$ $\forall λ ≥ 0$. Equivalentemente, $A(x+λd)= b$ y $x+λd ≥ 0$ $\forall λ ≥ 0$. Esto se cumple sii:
\[ \underset{=0}{Ax}+λAd = b \Leftrightarrow Ad = 0\]
\[ x + λd ≥ 0 \forall λ ≥ 0 \Leftrightarrow d ≥ 0 \]
\end{dem}

\begin{theorem}[Teorema de caracterización de direcciones extremas]\label{carac-direcciones}
$d$ es una dirección extrema de $P$ sii $d = \begin{pmatrix}-B^{-1}a_j\\e_j\end{pmatrix}$ donde $B \subset A$, $B \in R^{m\times m}$, $rg(B) = m$ y $a_j \notin B$ tal qe $B^{-1}a_j ≤ 0$. (Demostrado en uno de los libros de Bazaraa)
\end{theorem}

\begin{nota} El número de direcciones extremas de $P$ es finito y está acotado por $\begin{pmatrix}n\\m\end{pmatrix}(n-m)$.
\end{nota}

\begin{theorem}[Teorema de representación de poliedros de Minkowski-Weyl]
Sea $P$ un poliedro en forma estándar y sean $x^1,x²,\dots,x^k$ sus puntos extremos y $d^1,d^2,\dots,d^l$ sus direcciones extremas. Consideramos 
\[ S = \{x \in \R^n : x = \sum_{i=1}^k λ_i x^i + \sum_{j=1}^l μ_j d^j, \sum_{i=1}^k λ_i=1, λ_i ≥ 0, μ_j ≥ 0\ \forall i,j\} \]
Entonces $S = P$. Es decir, podemos representar cualquier poliedro en forma estándar queda determinado por sus extremos y direcciones extremas.
\end{theorem}

\begin{dem} Lo demostraremos por doble inclusión:
\begin{itemize}
	\item $(S \subseteq P)$ Sea $x \in S$, $x = \sum_{i=1}^k λ_i x^i + \sum_{j=1}^l μ_j d^j$ para algún $λ$ y $μ$.
	\[ Ax = A\left( \sum_{i=1}^k λ_i x^i + \sum_{j=1}^l μ_j d^j\right) =  \sum_{i=1}^k λ_i \underset{=b}{Ax^i} + \sum_{j=1}^l μ_j \underset{=0}{Ad^j} = \sum_{j=1}^l b = b \]
	\[ x = \sum_{i=1}^k \underset{≥0}{λ_i} \underset{≥0}{x^i} + \sum_{j=1}^l \underset{≥0}{μ_j}\underset{≥0}{d^j} ≥ 0 \]
	Luego $x \in P$
	
	\item $(P \subseteq S)$ Supongamos que existe $z \in P$ pero $z \notin S$. Como $S$ es un convexo cerrado, existe $p \in \R^n$, $α \in \R$ tales que $p'z > α$, $p'x ≤ α$ $\forall x$ (separación de $z$ y $S$). Tenemos
	\begin{enumerate}
		\item $p'd^j ≤ 0$ $\forall j=1,\dots,l$
		
		Si existiese $j$ tal que $p'd^j > 0$, consideamos el rayo $x + λd^j, λ ≥ 0$ para cualquier $x \in S$, pero entonces $p'(x+λd^j) = p'x + λp'd^j ≤ α$, pero $λp'd^j$ tiende a infinito cuando $λ$ tiende a infinito, luego $p'(x+λd^j)$ no puede estar acotado por ningún $α$.
		\item $p'z > α ≥ p'x^i$ $\forall i=1,\dots,k$
	\end{enumerate}
	Sea $\overline{x}$ el punto extremo tal que $p'\overline{x} = \max_i p'x^i$. Usando el teorema \ref{carac-extremos}: existe una reordenación tal que $\overline{x} = \begin{pmatrix}B^{-1}b\\0\end{pmatrix}$, $A = [B\ N]$, $x = \begin{bmatrix}x_B\\x_n\end{bmatrix}$, $z = \begin{bmatrix}z_B\\z_n\end{bmatrix} \in P$, $Az=b$. De $[B\ N]\begin{bmatrix}z_B\\z_N\end{bmatrix} = b$ obtenemos que $z_B = B^{-1}b-B^{-1}Nz_N$. Luego usando (2):
	\[ 0 < p'(z-\overline{x}) = (p_B'\ p_N') \left[\begin{pmatrix}B^{-1}b-B^{-1}Nz_N\\z_N\end{pmatrix}-\begin{pmatrix}B^{-1}b\\0\end{pmatrix}\right] =-p_B'B^{-1}Nz_N + p'z_N = (p_N'-p_B'B^{-1}N)z_N \]
	Al menos una de las coordenadas de $p_N'-p_B'B^{-1}N$ debe ser positiva. Sea $j$ tal que:
	\[ 0 < (p_N'-P_B'B^{-1}N)_j = p_j - p_B'B^{-1}a_j \]
	Llamemos $y_j\footnote{\url{https://www.youtube.com/watch?v=q6EoRBvdVPQ}} = B^{-1}a_j$ . Veamos que $y_j > 0$. Si $y_j ≤ 0$, $d = \begin{pmatrix}-B^{-1}a_j\\e_j\end{pmatrix}$ debe ser una dirección extrema por el teorema \ref{carac-direcciones}. Entonces $p'd = (p_B'\ p_N')\begin{pmatrix}-B^{-1}a_j\\e_j\end{pmatrix}=-p_B'B^{-1}a_j + p_j > 0$, pero esto contradice (1). Luego $y_j = B^{-1}a_j > 0$. Denotemos por $\overline{b}=B^{-1}b$. Sea $d = \begin{pmatrix}-y_j\\e_j\end{pmatrix}$ y $\hat{x} = \overline{x} + λd$. Veamos que $\hat{x} \in P$ si $λ = \frac{\overline{b}_r}{y_{jr}} = \min\{\frac{\overline{b}_i}{y_{ji}} : y_{ji} > 0\}$..
	\[ A\hat{x} = A\overline{x} + \frac{\overline{b}_r}{y_{r_j}} A d = b + \frac{\overline{b}_r}{y_{jr}} [B\ N] \begin{pmatrix}-B^{-1}a_j\\e_j\end{pmatrix} = b + \frac{\overline{b}_r}{y_{jr}}(-a_j + a_j) = b \]
	Si $i$ es una cooredanda de $B$ con $i \neq r$:
	\[ \hat{x}_i = (B^{-1}b)_i + \frac{\overline{b}_r}{y_{jr}}(-y_{ji}) = \overline{b}_i - \frac{\overline{b}_r}{y_{jr}}y_{ji} \]
	Si $y_{ji} < 0 \Rightarrow \hat{x} ≥ 0$ evidentemente. Si $y_{ji} > 0$, entonces $\overline{b}_i - \frac{\overline{b}_r}{y_{jr}}y_{ji} ≥ 0$, ya que $ \frac{\overline{b}_r}{y_{ji}} ≥ \frac{\overline{b}_r}{y_{jr}}$.
	
	Si $i = r$, el resultado es directo, ya que $\overline{b}_r - \frac{\overline{b}_r}{y_{jr}}y_{jr} = 0 ≥ 0$.
	
	Si $i$ es una coordenada que está en $N$, se tiene que $i \neq j$, luego:
	\[ \hat{x}_i = 0 +  \frac{\overline{b}_r}{y_{jr}}0 = 0 \]
	\[ \hat{x}_j = 0 +  \frac{\overline{b}_r}{y_{jr}}1 ≥ 0 \]
	
	Luego $\hat{x} \in P$. Además $\hat{x}$ es un punto extremo representado por las columnas $\hat{B}= (a_1,\dots,a_m)$ y además verifica:
	\[ p'\hat{x} = p'(\overline{x} + \frac{\overline{b}_r}{y_{jr}}d) = p'\overline{x} + \underset{>0}{\frac{\overline{b}_r}{y_{jr}}} \underset{≥0}{p'd} > p'\overline{x} \]
	Esto contradice que $p'\overline{x}$ fuera máximo. Por reducción al absurdo, llegamos a que $z \in S$, lo que acaba la demostración.
	$\QED$
\end{itemize}
\end{dem}
\end{document}